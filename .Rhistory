# happens to the coefficients?
fit3 <- glm(I(1 - newUse) ~ as.factor(wind) - 1,
data=shuttle, family="binomial")
summary(fit3)$coef # -0.2513144 -0.2831263
summary(fit)$coef # 0.2513144 0.2831263
# The coefficients reverse their signs.
# Problem 4.
fit <- glm(count ~ spray - 1, data=InsectSprays, family="poisson")
summary(fit)$coef
rate <- exp(coef(fit))
rate[1] / rate[2] # 0.9456522
# Problem 5.
fit <- glm(count ~ as.factor(spray) + offset(log(count+1)),
family="poisson", data=InsectSprays)
fit2 <- glm(count ~ as.factor(spray) + offset(log(10)+log(count+1)),
family="poisson", data=InsectSprays)
summary(fit)$coef
summary(fit2)$coef
# as.factor(spray)B  0.003512473
# The coefficient estimate is unchanged
# Problem 6.
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54,
3.87, 4.97)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
install.packages("swirl")
library(swirl)
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
egrline <- lm(child ~
| parent, galton)
egrline <- lm(child ~ parent, galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
fit <- lm(child ~ parent, galton)
summary(fit)
fit$residuals
mean(fit$residuals)
cov(fit$residuals, galton$parent)
fit$coef[1]
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
quit()
# Quiz 2
# Problem 1.
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
testIndex = createDataPartition(diagnosis, p=0.50, list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p=3/4)[[1]]
training = mixtures[inTrain,]
testing = mixtures[-inTrain,]
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
# No relation between the outcome and other variables
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
# Step-like pattern -> 4 categories
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
# Another way
library(plyr)
splitOn <- cut2(training$Age, g=4)
splitOn <- mapvalues(splitOn,
from=levels(factor(splitOn)),
to=c("red", "blue", "yellow", "green"))
plot(training$CompressiveStrength, col=splitOn)
# There is a step-like pattern in the plot of outcome versus index
# in the training set that isn't explained by any of the predictor
# variables so there may be a variable missing.
# Problem 3.
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p=3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(Superplasticizer, data=training) # OR
ggplot(data=training, aes(x=Superplasticizer)) + geom_histogram() + theme_bw()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation # 9
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
acc1 <- C1$overall[1]
acc1 # Non-PCA Accuracy: 0.65
modelFit <- train(training$diagnosis ~ .,
method="glm",
preProcess="pca",
data=training,
trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
acc2 <- C2$overall[1]
acc2 # PCA Accuracy: 0.72
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.8,
outcome=training$diagnosis)
preProc$rotation # 9
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation # 9
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
data <- segmentationOriginal
set.seed(125)
inTrain <- data$Case == "Train"
trainData <- data[inTrain, ]
testData <- data[!inTrain, ]
cartModel <- train(Class ~ ., data=trainData, method="rpart")
cartModel$finalModel
plot(cartModel$finalModel, uniform=T)
text(cartModel$finalModel, cex=0.8)
library(pgmm)
data(olive)
dim(olive)
head(olive)
olive <- olive[,-1]
treeModel <- train(Area ~ ., data=olive, method="rpart2")
treeModel
newdata <- as.data.frame(t(colMeans(olive)))
predict(treeModel, newdata) # 2.875
treeModel <- train(Area ~ ., data=olive, method="rpart2")
treeModel
newdata <- as.data.frame(t(colMeans(olive)))
predict(treeModel, newdata) # 2.875
olive <- olive[,-1]
treeModel <- train(Area ~ ., data=olive, method="rpart")
treeModel <- train(Area ~ ., data=olive, method="rpart2")
library(pgmm)
data(olive)
dim(olive)
head(olive)
olive <- olive[,-1]
treeModel <- train(Area ~ ., data=olive, method="rpart")
treeModel
newdata <- as.data.frame(t(colMeans(olive)))
predict(treeModel, newdata) # 2.875
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train <- sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA <- SAheart[train,]
testSA <- SAheart[-train,]
set.seed(13234)
logitModel <- train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl, data=trainSA, method="glm",
family="binomial")
logitModel
missClass <- function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predictTrain <- predict(logitModel, trainSA)
predictTest <- predict(logitModel, testSA)
# Training Set Misclassification rate
missClass(trainSA$chd, predictTrain) # 0.2727273
# Test Set Misclassification rate
missClass(testSA$chd, predictTest) # 0.3116883
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
library(randomForest)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
testing <- read.csv2(file = "pml-testing.csv"), header = TRUE, na.strings=c("", "NA"))
testing <- read.csv2(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
setwd("~/Google Drive/Data Science - Johns Hopkins/Practical Machine Learning/Project")
testing <- read.csv2(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
training <- read.csv2(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing <- read.csv2(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
View(testing)
training <- read.csv(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing <- read.csv(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
View(testing)
##
## Load packages
##
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
set.seed(1)
##
## Loading data
##
training_df <- read.csv(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing_df <- read.csv(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
install.packages("rpart.plot")
install.packages("rattle")
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
set.seed(1)
testing_df$X
View(testing_df)
training <- read.csv(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing <- read.csv(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
##
## Cleaning
##
poor_coverage <- sapply(training, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
training.clean <- training[,poor_coverage==FALSE]
View(training.clean)
summary(training.clean$classe)
table(training.clean$classe)
library(reshape)
library("reshape2", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(reshape2)
numeric_cols <- sapply(training.clean, is.numeric)
training.clean.lng <- melt(training.clean[,numeric_cols], id="is_bad")
training.clean.lng <- melt(training.clean[,numeric_cols], id="classe")
View(training.clean)
training.clean[,numeric_cols]
head(training.clean[,numeric_cols])
typeof(training.clean$classe)
training.clean$classe <- as.factor(training.clean$classe)
numeric_cols <- sapply(training.clean, is.numeric)
# Melt dataset using reshape
training.clean.lng <- melt(training.clean[,numeric_cols], id="classe")
training.clean <- training[,low_coverage==FALSE]
training <- read.csv(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing <- read.csv(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
##
## Clean variables with low coverage (mostly NA fields)
##
low_coverage <- sapply(training, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
training.clean <- training[,low_coverage==FALSE]
training.clean$classe <- as.numeric(training.clean$classe)
numeric_cols <- sapply(training.clean, is.numeric)
# Melt dataset using reshape
training.clean.lng <- melt(training.clean[,numeric_cols], id="classe")
head(training.clean.lng)
#plot the distribution for bads and goods for each variable
p <- ggplot(aes(x=value, group=classe, colour=factor(classe)), data=training.clean.lng)
#quick and dirty way to figure out if you have any good variables
p + geom_density() +
facet_wrap(~variable, scales="free")
training.clean$classe <- as.factor(training.clean$classe)
View(training.clean)
View(training)
training <- read.csv(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing <- read.csv(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
##
## Clean variables with low coverage (mostly NA fields)
##
low_coverage <- sapply(training, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
training.clean <- training[,low_coverage==FALSE]
##
## EDA for feature selection
##
# Select numeric fields
training.clean2 <- training.clean
training.clean2$classe <- as.numeric(training.clean2$classe)
numeric_cols <- sapply(training.clean2, is.numeric)
# Melt dataset using reshape
training.clean.lng <- melt(training.clean2[,numeric_cols], id="classe")
head(training.clean.lng)
# Plot the distribution for each classe in a given variable
p <- ggplot(aes(x=value, group=classe, colour=factor(classe)), data=training.clean.lng)
p + geom_density() +
facet_wrap(~variable, scales="free")
rm(training.clean2)
View(training.clean)
##
## Splitting the training set
##
training.clean$classe <- as.factor(training.clean$classe)
idx <- runif(nrow(df.term)) > 0.75
training.clean$classe <- as.factor(training.clean$classe)
idx <- runif(nrow(training.clean)) > 0.75
train <- training.clean[idx==FALSE,]
test <- training.clean[idx==TRUE,]
rf_model<-train(classe~.,data=train,method="rf",
trControl=trainControl(method="cv",number=5),
prox=TRUE,allowParallel=TRUE)
rm(training.clean.lng)
ada_model <- train(classe~.,data=train,method="ada",
prox=FALSE,allowParallel=TRUE)
training.clean$classe <- as.factor(training.clean$classe)
idx <- runif(nrow(training.clean)) > 0.6
train <- training.clean[idx==FALSE,]
test <- training.clean[idx==TRUE,]
rf_model <- train(classe~.,data=train,method="rf",
prox=FALSE,allowParallel=TRUE)
ctree_model <- train(classe~.,data=train,method="rpart",
prox=FALSE,allowParallel=TRUE)
print(ctree_model)
View(train)
View(testing)
View(training.clean)
training.clean <- training.clean[,-c(1:5)]
idx <- runif(nrow(training.clean)) > 0.6
train <- training.clean[idx==FALSE,]
test <- training.clean[idx==TRUE,]
summary(train)
ctree_model <- train(classe~.,data=train,method="rpart",allowParallel=TRUE)
ctree_model <- train(factor(classe)~.,data=train,method="rpart",allowParallel=TRUE)
training.clean <- training.clean[complete.cases(training.clean), ]
idx <- runif(nrow(training.clean)) > 0.6
train <- training.clean[idx==FALSE,]
test <- training.clean[idx==TRUE,]
rf_model <- randomForest(classe~.,data=train, mtry=2, ntree=100)
rf_model
rf_model.pred = prediction(rf_model, test)
rf_model.pred = prediction(rf_model, test$classe)
library(ROCR)
install.packages("ROCR")
library(ROCR)
rf_model.pred = prediction(rf_model, test$classe)
rf_model.pred = prediction(rf_model, test)
rf_model <- randomForest(classe~.,data=train, mtry=2, ntree=500, na.action=na.omit)
rf_model.pred = prediction(rf_model, test)
rf_model.predict = predict(rf_model,type="prob",newdata=test)[,2]
rf_model.pred = prediction(rf_model, test$classe)
rf_model.predict
dim(rf_model.predict)
len(rf_model.predict)
length(rf_model.predict)
rf_model.pred = prediction(rf_model.predict, test$classe)
rf_model.predict = predict(rf_model,type="prob",newdata=test)[,2]
rf_model.pred = prediction(rf_model.predict, test$classe)
rf_model
rf_model$err.rate
rf_model$confusion
print(rf_model)
rf_model.predict = predict(rf_model,type="prob",newdata=test)[,2]
threshold <- 0.5
pred      <- factor( ifelse(rf_model.predict[, "yes"] > threshold, "yes", "no") )
rf_model.predict
typeof(rf_model.predict)
class(rf_model.predict)
pred  <- factor( ifelse(rf_model.predict[, "yes"] > threshold, "yes", "no") )
pred  <- factor( ifelse(rf_model.predict > threshold, "yes", "no") )
pred      <- relevel(pred, "yes")   # you may or may not need this; I did
confusionMatrix(pred, test$classe)
pred
rf_model.predict
pred  <- as.factor( ifelse(rf_model.predict > threshold, "yes", "no") )
confusionMatrix(pred, test$classe)
table(pred, test$classe)
pred  <- ifelse(rf_model.predict > threshold, TRUE, FALSE)
pred
rf_model.predict = predict(rf_model,type="reponse",newdata=test)[,2]
rf_model.predict = predict(rf_model,type="response",newdata=test)[,2]
rf_model.predict = predict(rf_model,type="vote",newdata=test)[,2]
rf_model.predict
rf_model.predict = predict(rf_model,type="response",newdata=test)[,2]
rf_model.predict = predict(rf_model,newdata=test)
rf_model.predict
confusionMatrix(rf_model.predict, test$classe)
predictions <- predict(rf_model, testing)
View(testing)
predictions <- predict(rf_model, testing, type="class")
training <- read.csv(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing <- read.csv(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
low_coverage <- sapply(training, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
training.clean <- training[,low_coverage==FALSE]
testing.clean <- testing[,low_coverage==FALSE]
predictions <- predict(rf_model, testing)
idx <- runif(nrow(training.clean)) > 0.6
train <- training.clean[idx==FALSE,]
test <- training.clean[idx==TRUE,]
rf_model <- randomForest(classe~.,data=train, mtry=2, ntree=500, na.action=na.omit)
print(rf_model)
rf_model.predict = predict(rf_model,newdata=test)
confusionMatrix(rf_model.predict, test$classe)
predictions <- predict(rf_model, testing)
View(test)
names(test)
names(testing.clean)
names(test)
names(testing.clean)
predictions <- predict(rf_model, testing)
testing[,-1]
testing[-1,]
testing[1:59,]
testing[,1:59]
names(testing.clean)
names(testing[,1:59])
names(testing.clean[,1:59])
predictions <- predict(rf_model, testing.clean)
predictions <- predict(rf_model, testing.clean[,1:59])
names(testing.clean[,1:59])
names(testing.clean[,5:59])
levels(test)
levels(test$X)
levels(test$pitch_belt)
levels(training.clean)
levels(training.clean$classe)
levels(testing.clean) <- levels(test)
predictions <- predict(rf_model, newdata=testing.clean)
predictions <- predict(rf_model, newdata=testing.clean[,1:59])
predictions <- predict(rf_model, newdata=testing)
training <- read.csv(file = "pml-training.csv", header = TRUE, na.strings=c("", "NA"))
testing <- read.csv(file = "pml-testing.csv", header = TRUE, na.strings=c("", "NA"))
##
## Clean variables with low coverage (mostly NA fields)
##
low_coverage <- sapply(training, function(x) {
coverage <- 1 - sum(is.na(x)) / length(x)
coverage < 0.8
})
training.clean <- training[,low_coverage==FALSE]
testing.clean <- testing[,low_coverage==FALSE]
idx <- runif(nrow(training.clean)) > 0.6
train <- training.clean[idx==FALSE,]
test <- training.clean[idx==TRUE,]
rf_model <- randomForest(factor(classe)~.,data=train, mtry=2, ntree=500, na.action=na.omit)
training$classe <- as.factor(training$classe)
testing$problem_id$ <- as.factor(testing$problem_id)
testing$problem_id <- as.factor(testing$problem_id)
rf_model.predict = predict(rf_model,newdata=test)
confusionMatrix(rf_model.predict, test$classe)
predictions <- predict(rf_model, newdata=testing)
predictions <- predict(rf_model, newdata=testing.clean)
head(testing.clean[,-1])
head(testing.clean[-1,])
head(testing.clean[1:59,])
head(testing.clean[,1:59])
head(testing.clean[,1:59])
predictions <- predict(rf_model, newdata=testing.clean[,1:59])
